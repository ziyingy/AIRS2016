\section{Introduction}
\label{sec-intro}

Batch evaluation techniques are widely used in information retrieval
system measurement.
Each system that is to be compared generates a ranking, or
{\emph{run}}, for each of a set of topics, with documents included in
the run and also ordered within the run on the basis of some computed
textual {\emph{similarity score}} relative to the given query.
Possible similarity computations include the Okapi BM25 mechanism of
{\citet{rwjhg94trec}} and the language modeling techniques of
{\citet{pc98sigir}}.
Static score components such as Pagerank or other assessments of
document quality can also be included.
Those runs are then mapped to numeric {\emph{effectiveness values}}
using a set of relevance judgments and an {\emph{effectiveness
metric}}, which generates a single number as an assessment of the
quality, or utility, of that run in the eyes of the user that is
presumed to have inspected it.
Finally, the effectiveness values are aggregated in some way across
topics to get an overall performance measure which is often used,
with a suitable statistical test, as a basis for answering the
question ``is System A demonstrably better than System~B?''.

In this work we consider the consequences of allowing {\emph{tied
similarity scores}} (or just {\emph{ties}}) in the ranking.
The obvious issue is that ties admit a level of ambiguity in the
effectiveness metric values, and hence (potentially) in the outcome
of a system versus system comparison, since a group of documents that
all share the same computed similarity score could be presented to
the user in any permutation that is consistent with the scores being
non-increasing.
Our first goal is thus to quantify the extent to which past Text
Retrieval Conference (TREC) evaluation exercises have been affected
by tied similarity scores, and determine whether the presence of
ties may have caused ambiguity to flow through into system
scores.
In this part of the project we make use of a range of tie-breaking
regimes, including the rules embedded in the well-known
{\treceval} program, and conclude that while ties have had the
potential to be significantly disruptive, in practice they did not
influence the outcomes of the measurements that were undertaken.

A second related goal is to ask whether the deliberate introduction
of ties might be useful in some way.
For example, a range of approaches in which similarity scoring might
be approximated or otherwise quantized have been suggested over the
years including, for example
the quantized document weights of {\citet{mzs94ipm}}, or
the impact-ordered indexes of {\citet{am06sigir}}.
If we allow that the retrieval system might gain tangible efficiency
benefits from assigning scores with low precision to
documents,
%% or even to deliberately choose the precision to which
%% particular documents are scored,
then we may end up with large
numbers of ties in the runs that the system generates,
and being able to estimate the extent to which ties can
be tolerated before there is risk of degraded system retrieval
effectiveness is a key component of the approximation.
In experiments using submitted TREC runs, we show that quite marked
levels of approximation can be tolerated before system scores change
significantly, and hence that relatively low-precision scoring can be
employed if it boosts efficiency.

%% {\todo{Macros to be used for leaving messages through the paper:}}
%% \alicia{done with {\tt{alicia}}.}
%% \andrew{done with {\tt{andrew}}.}
%% \alistair{done with {\tt{alistair}}.}
