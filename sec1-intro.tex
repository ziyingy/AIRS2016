\section{Introduction}
\label{sec-intro}

Batch evaluation techniques are widely used in information retrieval
system measurement.
Each system that is to be compared generates a ranking, or
{\emph{run}}, for each of a set of topics, with documents included in
the run and also ordered within the run on the basis of some computed
textual {\emph{similarity score}} relative to the given query.
Example similarity computations include the Okapi BM25 mechanism of
{\citet{rwjhg94trec}} and the language modeling techniques of
{\citet{pc98sigir}}.
Those runs are then mapped to numeric {\emph{effectiveness values}}
using a set of relevance judgments and an {\emph{effectiveness
metric}}, which generates a single number as an assessment of the
quality, or utility, of that run in the eyes of the user that is
presumed to have inspected it.
Those effectiveness values are then aggregated in some way across
topics to get an overall performance measure; or are used as the basis
of a statistical test to (hopefully) determine a question of the form
``is System A demonstrably better than System~B''.

In this paper we consider the consequences of allowing {\emph{tied
similarity scores}} (or just {\emph{ties}}) in the ranking.
One obvious issue is that ties admit a level of ambiguity in the
effectiveness values, and hence (potentially) in the outcome of a
system versus system comparison, since a group of documents that all
share the same computed similarity score could be presented to the
user in any permutation that is consistent with the scores being
non-increasing.
Our first goal is thus to quantify the extent to which past Text
Retrieval Conference (TREC) evaluation exercises have been affected
by ties, and determine whether the presence of ties may have caused
ambiguity to flow through to system scores.
In this part of the project we make use of a range of tie-breaking
regimes, including the rules embedded in the well-known
{\tt{trec\_eval}} program, and conclude that while ties had the
potential to be quite significantly disruptive, in practice they did
not influence the outcomes of the measurements that were undertaken.

A second and related goal is to then ask whether the deliberate
introduction of ties might be useful in some way.
For example, a range of ways in which similarity scoring might be
approximated or otherwise quantized have been suggested over the
years, including the impact-ordered indexes of {\citet{am06sigir}}.
If we allow that the retrieval system might gain tangible efficiency
benefits from being permitted to assign low precision scores to
documents, or even to deliberately choose the precision to which
particular documents are scored, then we may end up with large
numbers of ties in the runs that the system generates.
In this scenario, being able to estimate the extent to which ties can
be permitted before there is risk in incorrect effectiveness scores
being generated is a necessary precondition.
In experiments using submitted TREC runs, we show that
{\alistair{something}}.

{\todo{Macros to be used for leaving messages through the paper:}}
\alicia{done with {\tt{alicia}}.}
\andrew{done with {\tt{andrew}}.}
\alistair{done with {\tt{alistair}}.}
