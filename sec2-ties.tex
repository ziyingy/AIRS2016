\section{Ties, and Methods for Dealing With Them}
\label{sec-ties}

\myparagraph{Terminology}

We suppose that the similarity scores generated for a query partition
the document ranking -- the {\emph{run}} -- into {\emph{groups}} in
which all of the documents have the same score.
Let $b_g$ be the rank in the run at which the $g$\,th equi-score
group commences, with, by definition, $b_1=1$; and let $e_g$ be the
rank of the last document in that group, with $b_{g+1}=e_g+1$.
That is, the $g$\,th group of tied documents spans the items
$[b_g\ldots e_g]$, and contains $s_g=b_{g+1}-b_g$ documents.
We further define $G_g$ to be the
multiset of gain values in the $g$\,th group,
%%
	$G_g =\{r_k\mid b_g \le k \le e_g\}$, 
%%
with $0\le r_k \le 1$ the gain associated with the document at rank $k$;
and define $t_g$
to be the total gain associated with the $g$\,th group,
%%
	$t_g = \sum \{r_k\mid b_g \le k \le e_g\}$.
%%
For example, consider the ten-item ranking with scores shown in
Figure~\ref{fig-example}, with each document given a single letter
label for convenience, and with five different computed similarity
scores.
The second row shows a presumed relevance value for each
corresponding document (``$0$'' and ``1''); and the third row lists
the similarity scores that are presumed to have led to that ranking.

If the scores are ignored and only the list of relevance values is
employed, computation of (for example) the metric precision at depth
$k=5$ (P@$5$) yields a score of $2/5=0.4$, because there are two
``$1$''s among the first five gain values.
Similarly, the ranking shown has a reciprocal rank (RR) score of
$1/3=0.333$, since the first relevant document appears at rank $k=3$.
Other metrics such as average precision (AP), rank-biased precision
(RBP) {\citep{mz08acmtois}}, and normalized discounted cumulative
gain (NDCG) {\citep{jk02acmtois}}, can also be computed, based solely
on that third ``gain'' row, without consideration of the document
labels in the first row, or their scores in the second row.

\begin{figure}[t]
\centering
\input{fig-example.tex}
\caption{Example run showing five equi-score groups.
\label{fig-example}}
\end{figure}

When scores are included, the situation changes.
Now documents M and S can be seen to have the same similarity score,
and are part of a tied group.
That means that P@$5$ might be either $2/5$ or $3/5$, depending on
the tie-breaking rule employed to order them.
Similarly, RR might be $1/2$ or $1/3$, because of the tie involving
documents H and A and C (but note that there is no
possible arrangement in which RR can be $1/4$).

\myparagraph{Run Order}

A range of mechanisms have evolved to deal with tied scores.
The first and most obvious option is to do as has already been
suggested in connection with the example shown in
Figure~\ref{fig-example}, and that is to ignore the document scores
and process the run in the order in which the documents are presented
-- in effect, pushing the responsibility for tie-breaking back to the
retrieval system, whether or not it accepts it.
This approach presumes that the system has employed more information
than is captured in the final score, perhaps via further precision in
the internal computation above and beyond what is passed to the
evaluation regime, or perhaps via a secondary-key ordering process
that is not part of the scores at all.
However the system's ordering arises, respecting the sequential
presentation of documents is a plausible default way of handling tied
scores.

\myparagraph{External Tie-Break Rule}

A second option is to make use of some external fixed ordering
criterion and use it to reorder the documents within each tied group,
thereby obtaining a canonical representation for the run.
For example, the documents in each group might be sorted according to
their document identifier, or according to their length, or according
to their URL or filename.
As one specific example of this type of approach, the widely-used
{\tt{trec\_eval}} program (see
{\small\url{http://trec.nist.gov/trec_eval/}}) sorts tied groups into
decreasing order of document identifier before performing its various
effectiveness metric computations.

\myparagraph{Optimistic and Pessimistic Limits}

A third way of handling runs with ties is to compute the best and
worst scores that might arise, and then present a score range rather
than a score value.
The advantage of this approach is that it makes clear when scores
contain potential ambiguity, in a way that mirrors the residuals of
{\citet{mz08acmtois}}, which provide guidance as to the metric weight
assigned to unjudged documents.
To compute an optimistic upper score bound, the $t_g$ relevant
documents within the $g$\,th group are assumed to appear in the
first rank positions, that is, $[b_g\ldots b_g+t_g-1]$, and the
metric score then computed in the usual way.
Similarly, to get a pessimistic lower score bound, the $t_g$ relevant
documents in the group are assumed to appear as a block as deep in
the run as is possible, at ranks $[e_g-t_g+1\ldots e_g]$.
In the example shown in Figure~\ref{fig-example}, the ordering ``H
then A then C'' (and similarly in the other groups) is used to derive
a lower bound on the score, and the ordering ``A then C then H'' (and
so on) is used to obtain an upper bound.
If a document is unjudged, then for many metrics (but notably, not
for AP or NDCG) it should be assumed to be non-relevant for the
purposes of establishing the lower bound, and assumed to be relevant
for the purposes of establishing the upper bound.

\myparagraph{Averaging Across Permutations}

While the worst-case bounds can be informative, they are also
somewhat pessimistic, and computing the average, or expected, value
of the metric across all possible permutations of documents within
each of the tied score groups provides a useful balance.
If every permutation of documents in each group is equally likely,
then computing the expectation is simply the process of computing the
metric for each permutation and taking their average.
For a small number of small groups, this $O(\prod_g (s_g!))$
brute-force approach is computationally feasible.
But if there are many blocks, or if there are any large blocks, it is
expensive.
Fortunately, the summation over all permutations telescopes for most
metrics, leading to a tractable computation.
{\citet{mn08ecir}} describe this process in detail, and present an
incremental formulation for average precision that computes the
expected score across all possible permutations of documents in each
group.
A similar computation can be used to compute an expected (across
permutations within groups) RR score.

For weighted-precision metrics such as RBP, a similar process can be
adopted.
The set of gain values associated with each group is summed and
averaged, and then that average gain applied at each rank position,
and weighted according to the decay function.
For the example shown in Figure~\ref{fig-example}, and an RBP
parameter $p=0.5$, the expected RBP0.5 score is computed as
\newcommand{\mysub}[2]{{#1}_{\mbox{\scriptsize{\,#2}}}}
\[
	%% \mbox{RBP0.5} = 
	0.5\times
		%% \frac{\mysub{0}{D}}{1} 
		\frac{0}{1}
	+
	(0.25+0.125+0.0625)\times
		%% \frac{\mysub{0}{H}+\mysub{1}{A}+\mysub{1}{C}}{3}
		\frac{2}{3}
	+
	(0.0313+0.0156)\times
		%% \frac{\mysub{0}{M}+\mysub{1}{S}}{2}
		\frac{1}{2}
	+
	\cdots
\]
We use these formulations for expected AP, expected RR (not to be
confused with the metric ERR), and expected RBP in the experiments
described in the next two sections.
