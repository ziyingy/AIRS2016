\section{Ties, and Methods for Dealing With Them}
\label{sec-ties}

\myparagraph{Terminology}

\begin{figure}[t]
\centering
\input{fig-example.tex}
\caption{Example run showing five equi-score groups.
\label{fig-example}}
\end{figure}

We suppose that the similarity scores generated for a query divide
the document ranking -- the {\emph{run}} -- into groups in which all
of the documents have the same score.
Let $b_g$ be the rank in the run at which the $g$\,th equi-score
group commences, with, by definition, $b_1=1$.
That is, the $g$\,th group of tied dcouments spans the items
$[b_g\ldots b_{g+1}-1]$, and contains $b_{g+1}-b_g$ documents.
For example, consider the run with scores shown in
Figure~\ref{fig-example}.
In this example, the top-$10$ document ranking (with documemts
labeled consecutively for convenience of reference) is assumed to
contain five different computed similarity scores.
The last row shows a presumed relevance value for each corresponding
document, with ``$0$'' indicating not relevant and ``$1$'' indicating
relevant.
If the scores are ignored and only the list of relevance values is
available, computation of the metric precision at depth $k=5$ (P@$5$)
yields a score of $2/5=0.4$, because there are two ``$1$''s among the
first five gain values.
Similarly, the ranking shown has a reciprocal rank (RR) score of
$1/3=0.333$, since the first relevant document appears at rank $k=3$.
Other metrics such as average precision (AP), rank-biased precision
(RBP), and normalized discounted cumulative gain (NDCG), can also be
computed, based solely on the ``gain'' row, without consideration of
the document labels or their actual scores.

When scores are include in the evaluation, the situation changes.
Now documents E and F in the example have the same similarity score,
and are part of the third tied group.
That means that P@$5$ might be either $2/5$ or $3/5$, depending on
the tie-breaking rule employed to order E and F.
Similarly, RR might be $1/2$ or $1/3$, because of the tie between B
and C and D (but cannot be $1/4$).

For notational convenience, we further define $e_g$ to be the rank of
the last document in the $g$\,th group, that is, $e_k=b_{k+1}-1$;
define $G_g$ to be the multiset of gain values in the $g$\,th group,
%%
	$G_g =\{r_k\mid b_g \le k \le e_g\}$, 
%%
and define $t_g$ to be the total gain associated with the $g$\,th
group,
%%
	$t_g = \sum \{r_k\mid b_g \le k \le e_g\}$.
%%
We now describe a range of ways in which runs with tied scores might
be evaluated.

\myparagraph{Run Order}

The first and most obvious option is to do as has already been
suggested in connection with the example shown in
Figure~\ref{fig-example}, and that is to ignore the document scores
and simply process the run in the order in which the documents are
presented by the retrieval system.
This approach presumes that the system itself will have employed more
information than is captured in the score that it provides, perhaps
via further precision in the internal computation above and beyond
what is passed to the evaluation regime, or perhaps via a
secondary-key ordering process that is not explicit in the final
computed scores.
However that ordering arises, it is clear that respecting the
sequential presentation of documents (and hence gains) generated by
the system is an importnat default way of handling tied scores.

\myparagraph{External Tie-Break Rule}

A second option is to make use of some external fixed ordering
criterion and use it to reorder the documents within each tied group
in order to obtain a canonical representation for the run.
For example, the documents in each group might be sorted according to
their document identifier, or according to their length, or according
to their URL or filename.
As one specific example of this type of approach, the widely-used
{\tt{trec\_eval}} program (see
{\small\url{http://trec.nist.gov/trec_eval/}}) sorts tied groups into
decreasing order of document identifier before performing its various
effectiveness metric computations.

\myparagraph{Optimistic and Pessimistic Limits}

A third option is to compute score ranges rather than score values,
so as to capture the possible variability 
\todo{more...}

\myparagraph{Averaging Across Permutations}

\todo{Discuss {\citet{mn08ecir}}, and describe basis for their formulations}
