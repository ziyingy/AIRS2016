\section{Ties, and Methods for Dealing With Them}
\label{sec-ties}

\myparagraph{Terminology}

We suppose that the similarity scores generated for a query divide
the document ranking -- the {\emph{run}} -- into groups in which all
of the documents have the same score.
Let $b_g$ be the rank in the run at which the $g$\,th equi-score
group commences, with, by definition, $b_1=1$, and let $e_g$ be the
rank of the last document in that group, with $b_{g+1}=e_g+1$.
That is, the $g$\,th group of tied documents spans the items
$[b_g\ldots e_g]$, and contains $b_{g+1}-b_g$ documents.
For example, consider the run with scores shown in
Figure~\ref{fig-example}.
This example shows a ranled list of 10 documents, each with 
single letter labels for convenience, with
five different computed similarity scores.
The second row shows a presumed relevance value for each
corresponding document, with ``$0$'' indicating not relevant and
``$1$'' indicating relevant; and the third row lists the similarity
scores that are presumed to have led to that ranking.

If the scores are ignored and only the list of relevance values is
available, computation of (for example) the metric precision at depth
$k=5$ (P@$5$) yields a score of $2/5=0.4$, because there are two
``$1$''s among the first five gain values.
Similarly, the ranking shown has a reciprocal rank (RR) score of
$1/3=0.333$, since the first relevant document appears at rank $k=3$.
Other metrics such as average precision (AP), rank-biased precision
(RBP), and normalized discounted cumulative gain (NDCG), can also be
computed, based solely on the ``gain'' row, without consideration of
the document labels or their actual scores.

\begin{figure}[t]
\centering
\input{fig-example.tex}
\caption{Example run showing five equi-score groups.
\label{fig-example}}
\end{figure}

When scores are included in the evaluation, the situation changes.
Now documents M and S can be seen to have the same similarity score,
and are part of a tied group.
That means that P@$5$ might be either $2/5$ or $3/5$, depending on
the tie-breaking rule employed to order them.
Similarly, RR might be $1/2$ or $1/3$, because of the tie involving
documents H and A and C (but cannot be $1/4$).

For notational convenience, we further define $G_g$ to be the
multiset of gain values in the $g$\,th group,
%%
	$G_g =\{r_k\mid b_g \le k \le e_g\}$, 
%%
define $s_g=e_g-b_g+1$ to be the size of that group, and define $t_g$
to be the total gain associated with the $g$\,th group,
%%
	$t_g = \sum \{r_k\mid b_g \le k \le e_g\}$.
%%
We now describe a range of ways in which runs with tied scores might
be evaluated.

\myparagraph{Run Order}

The first and most obvious option is to do as has already been
suggested in connection with the example shown in
Figure~\ref{fig-example}, and that is to ignore the document scores
and simply process the run in the order in which the documents are
presented by the retrieval system.
This approach presumes that the system itself will have employed more
information than is captured in the score that it provides, perhaps
via further precision in the internal computation above and beyond
what is passed to the evaluation regime, or perhaps via a
secondary-key ordering process that is not explicit in the final
computed scores.
However that ordering arises, it is clear that respecting the
sequential presentation of documents (and hence gains) generated by
the system is an importnat default way of handling tied scores.

\myparagraph{External Tie-Break Rule}

A second option is to make use of some external fixed ordering
criterion and use it to reorder the documents within each tied group
in order to obtain a canonical representation for the run.
For example, the documents in each group might be sorted according to
their document identifier, or according to their length, or according
to their URL or filename.
As one specific example of this type of approach, the widely-used
{\tt{trec\_eval}} program (see
{\small\url{http://trec.nist.gov/trec_eval/}}) sorts tied groups into
decreasing order of document identifier before performing its various
effectiveness metric computations.

\myparagraph{Optimistic and Pessimistic Limits}

A third way of scoring runs with ties is to compute the best and
worst scores that might arise, and then present a score range rather
than a score value.
The advantage of this approach is that it makes clear when scores
contain potential ambiguity, in the same way that the residuals of
{\citet{mz08acmtois}} provide guidance as to the metric weight
assigned to unjudged documents.
To compute an optimistic upper score bound, the $t_g$ relevant
documents within the $g$\,th group should be assumed to appear in the
first rank positions, that is, $[b_g\ldots b_g+t_g-1]$, and the
metric score then computed in the usual way.
Similarly, to get a pessimistic lower score bound, the $t_g$ relevant
documents in the group are assumed to appear as a block as deep in
the run as is possible, at ranks $[e_g-t_g+1\ldots e_g]$.
In the example shown in Figure~\ref{fig-example}, the ordering ``H
then A then C'' (and similarly in the other groups) is used to derive
a lower bound on the score, and the ordering ``A then C then H'' (and
so on) is used to obtain an upper bound.
If a document is unjudged, then for many metrics (but notably, not
for AP or NDCG) it should be assumed to be non-relevant for the
purposes of establishing the lower bound, and assumed to be relevant
for the purposes of establishing the upper bound.

\myparagraph{Averaging Across Permutations}
While the extreme limits of the previous method can be informative, 
computing the average, or expected, value of a metric across all possible 
permutations of documents within a tied block of scores is also useful.
If we assume that every permutation of values is equally likely, 
then computing the expectation is simply the process of computing the 
metric for each permutation and taking their average.
For small blocks of $n$ documents, this $O(n!)$ brute force approach 
is computationally feasible, but 
for larger blocks it will take too long.
Fortunately the summation over all permutations telescopes for most metrics, 
leading to a more tractable algorithm for computation.

\todo{Discuss {\citet{mn08ecir}}, and describe basis for their formulations}
