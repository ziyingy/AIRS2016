\section{Conclusion and Future Work}
\label{sec-conclusion}

We have explored the impact of score ties on the evaluation of
retrieval system effectiveness, as measured using binary relevance
judgments and three established effectiveness metrics.
Ties have the potential to affect system comparisons, and using TREC
data, we showed that a small number of systems did indeed generate
runs with very ambiguous score outcomes, but that -- fortunately --
the overall conclusions from those rounds of experimentation were
unlikely to have been compromised.
We further demonstrated that allowing a controlled grouping of scores
in runs -- in a sense, permitting the deliberate introduction of ties
-- resulted in only small changes in the ability to compare systems.
This approach represents a novel direction in which retrieval
efficiency improvements might be achieved.
We have not yet addressed the question of how those efficiency gains
might be achieved, and a clear direction for future work is to
reexamine the computation embedded in standard similarity scoring
regimes and existing dynamic pruning heuristics, to identify and
measure ways in which processing economies might accrue through the
use of inexact scoring.

Another area for future work is in the space of test collection
construction.
Previous investigations
{\citep{voorhees00ipm,bcstvy08sigir,sst11sigir}} have explored the
reliability and quality of the collected judgments; it may be that
the pooled documents can be stratified according to the groups they
appear in, and less emphasis placed on judgment quality for deeper
pools, relying instead on averaging effects to preserve overall
evaluation quality.

%% \andrew{From {\citet{sst11sigir}}: Ever since the use of test
%% collections became widely known, criticisms of the approach were
%% described in the lit- erature.
%% In 1968, {\cite{katter68isr}} wrote that ``a recurring finding from
%% studies involving relevance judgments is that the inter- and
%% intra-judge reliability of relevance judgments is not very high''.
%% Early test collection proponents, {\citet{cleverdon70report}} and
%% {\citet{ls68isr}} both published studies comparing judgments by
%% different assessors.
%% They each concluded that despite a high level of disagreement between
%% the assessors, the relative ranking of IR systems was largely
%% unaffected by which set of judgments was used.
%% These studies were repeated on a larger scale in a later study by
%% {\citet{voorhees00ipm}}, in general drawing broadly similar
%% conclusions, although {\citet{bcstvy08sigir}} showed that poor
%% assessment can affect some aspects of measuring IR systems
%% accurately.}
